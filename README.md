# Subreddit Statistics ETL
This ETL pipeline analyzes user activity on a subreddit dedicated to the Cormoran Strike novels by Robert Galbraith. It can be easily customized for a different subreddit by adjusting a few parameters.

## Table of Contents
- [Overview](#overview)
- [Tech Stack and Architecture](#tech-stack-and-architecture)
- [Getting Started](#getting-started)
  - [Prerequisites](#prerequisites)
  - [Configuration](#configuration)
  - [Installation](#installation)
- [Usage](#usage)
- [Testing](#testing)


## Overview
This is very basic, but very common data pipeline that illustrates the main steps found in many workflows:
  - read data from external source and load it to staging area
  - clean data and check errors
  - apply transformations and load data to OLAP Data Warehouse 
  - visualizing data insights
![image](https://github.com/user-attachments/assets/26525067-d92d-41f6-9d97-6554fbf7aa44)

## Tech Stack and Architecture
<img width="899" alt="Screenshot 2025-01-25 at 9 17 20â€¯PM" src="https://github.com/user-attachments/assets/7f85cba4-2e6d-48b3-ba5a-3875a58751b9" />

+ Extract data using Reddit API
+ Load into AWS S3
+ Copy to AWS Redshift
+ Visualize with Looker Studio Dashboard
+ Orchestrate with Airflow in Docker


## Getting Started
### Prerequisites
- docker to run airflow
- AWS S3 bucket
- AWS redshift instance
- Looker studio dashboard template
  
### Configuration
User has to set up environmental variables to connect:
- Reddit API
- AWS S3
- AWS Redshift

The required  dependencies are defined in docker-compose.yaml file as PIP_ADDITIONAL_REQUIREMENTS

### Installation
1. Clone the repository  
2. Add your .env file with environment variables (refer to the configuration section for details).
3. Start the Docker containers using the docker-compose.yaml file:
   ```bash
   docker-compose up
4. Trigger airflow dag manually or schedule it

## Usage
+ ETL pipeline runs weekly orchestrated by airflow, getting subreddit data and saving it to AWS S3 storage and AWS Redshidt tables
To get historic information user can use airflow backfilling command
   ```bash
    docker-compose exec airflow-scheduler airflow dags backfill -s 2023-01-01 -e 2024-10-31 reddit_pipeline_dag

+ To customize this project for another subreddit, you need to change the subreddit's name and the words of interest variables in src/config.py to your desired values.

## Testing
The airflow task is configured to execute the test.py file, which contains all the test cases written using the pytest framework.
   ```bash
      pytest src/test/test_reddit_etl.py





   
